{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bit1d825da1f8bf4e7a8326c44dcf9005aa",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "False"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cpu\n"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_folder = 'transformers/Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = pd.read_csv(\"data/news.csv\")\n",
    "true_dataset =  pd.read_csv(\"data/True.csv\")\n",
    "fake_dataset = pd.read_csv(\"data/Fake.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining two datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_string(x):\n",
    "\n",
    "    x = x.split(maxsplit=200)\n",
    "    x = ' '.join(x[:200])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_list=[]\n",
    "for index, article in news_dataset.iterrows():\n",
    "    new_list.append({'title':article[\"title\"],'text':trim_string(article[\"text\"]),'full':trim_string(article[\"title\"]+\". \"+article[\"text\"]),'label':1 if article[\"label\"]==\"REAL\" else 0})\n",
    "\n",
    "for index, article in true_dataset.iterrows():\n",
    "    new_list.append({'title':article[\"title\"],'text':trim_string(article[\"text\"]),'full':trim_string(article[\"title\"]+\". \"+article[\"text\"]),'label':1})\n",
    "\n",
    "for index, article in fake_dataset.iterrows():\n",
    "    new_list.append({'title':article[\"title\"],'text':trim_string(article[\"text\"]),'full':trim_string(article[\"title\"]+\". \"+article[\"text\"]),'label':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(new_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   title  \\\n0                           You Can Smell Hillary’s Fear   \n1      Watch The Exact Moment Paul Ryan Committed Pol...   \n2            Kerry to go to Paris in gesture of sympathy   \n3      Bernie supporters on Twitter erupt in anger ag...   \n4       The Battle of New York: Why This Primary Matters   \n...                                                  ...   \n51228  McPain: John McCain Furious That Iran Treated ...   \n51229  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...   \n51230  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...   \n51231  How to Blow $700 Million: Al Jazeera America F...   \n51232  10 U.S. Navy Sailors Held by Iranian Military ...   \n\n                                                    text  \\\n0      Daniel Greenfield, a Shillman Journalism Fello...   \n1      Google Pinterest Digg Linkedin Reddit Stumbleu...   \n2      U.S. Secretary of State John F. Kerry said Mon...   \n3      — Kaydee King (@KaydeeKing) November 9, 2016 T...   \n4      It's primary day in New York and front-runners...   \n...                                                  ...   \n51228  21st Century Wire says As 21WIRE reported earl...   \n51229  21st Century Wire says It s a familiar theme. ...   \n51230  Patrick Henningsen 21st Century WireRemember w...   \n51231  21st Century Wire says Al Jazeera America will...   \n51232  21st Century Wire says As 21WIRE predicted in ...   \n\n                                                    full  label  \n0      You Can Smell Hillary’s Fear. Daniel Greenfiel...      0  \n1      Watch The Exact Moment Paul Ryan Committed Pol...      0  \n2      Kerry to go to Paris in gesture of sympathy. U...      1  \n3      Bernie supporters on Twitter erupt in anger ag...      0  \n4      The Battle of New York: Why This Primary Matte...      1  \n...                                                  ...    ...  \n51228  McPain: John McCain Furious That Iran Treated ...      0  \n51229  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...      0  \n51230  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...      0  \n51231  How to Blow $700 Million: Al Jazeera America F...      0  \n51232  10 U.S. Navy Sailors Held by Iranian Military ...      0  \n\n[51233 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n      <th>full</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>You Can Smell Hillary’s Fear</td>\n      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n      <td>You Can Smell Hillary’s Fear. Daniel Greenfiel...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kerry to go to Paris in gesture of sympathy</td>\n      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n      <td>Kerry to go to Paris in gesture of sympathy. U...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The Battle of New York: Why This Primary Matters</td>\n      <td>It's primary day in New York and front-runners...</td>\n      <td>The Battle of New York: Why This Primary Matte...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>51228</th>\n      <td>McPain: John McCain Furious That Iran Treated ...</td>\n      <td>21st Century Wire says As 21WIRE reported earl...</td>\n      <td>McPain: John McCain Furious That Iran Treated ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>51229</th>\n      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-ac...</td>\n      <td>21st Century Wire says It s a familiar theme. ...</td>\n      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-ac...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>51230</th>\n      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to T...</td>\n      <td>Patrick Henningsen 21st Century WireRemember w...</td>\n      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to T...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>51231</th>\n      <td>How to Blow $700 Million: Al Jazeera America F...</td>\n      <td>21st Century Wire says Al Jazeera America will...</td>\n      <td>How to Blow $700 Million: Al Jazeera America F...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>51232</th>\n      <td>10 U.S. Navy Sailors Held by Iranian Military ...</td>\n      <td>21st Century Wire says As 21WIRE predicted in ...</td>\n      <td>10 U.S. Navy Sailors Held by Iranian Military ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>51233 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = np.split(dataset.sample(frac=1), [int(.6*len(dataset)), int(.8*len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train.csv\", index=False)\n",
    "validate.to_csv(\"validate.csv\", index=False)\n",
    "test.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Token indices sequence length is longer than the specified maximum sequence length for this model (864 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (883 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (704 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (719 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (633 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (852 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (846 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (4228 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (4260 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (957 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (852 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (633 > 512). Running this sequence through the model will result in indexing errors\n"
    }
   ],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "\n",
    "fields = [('title', text_field), ('text', text_field), ('full', text_field),('label', label_field)]\n",
    "train, valid, test = TabularDataset.splits(path=\"\",train=\"train.csv\", validation=\"validate.csv\", test=\"test.csv\", format='CSV', fields=fields, skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text), train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.text), train=True, sort=True, sort_within_batch=True)\n",
    "test_iter = Iterator(test, batch_size=16, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        options_name = \"bert-base-uncased\"\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n",
    "\n",
    "    def forward(self, text, label):\n",
    "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
    "\n",
    "        return loss, text_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "\n",
    "def save_checkpoint(save_path, model, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(load_path, model):\n",
    "    \n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion = nn.BCELoss(),\n",
    "          train_loader = train_iter,\n",
    "          valid_loader = valid_iter,\n",
    "          num_epochs = 5,\n",
    "          eval_every = len(train_iter) // 2,\n",
    "          file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for (title, text, titletext,labels), _ in train_loader:\n",
    "            labels = labels.type(torch.LongTensor)           \n",
    "            labels = labels.to(device)\n",
    "            titletext = titletext.type(torch.LongTensor)  \n",
    "            titletext = titletext.to(device)\n",
    "            output = model(titletext, labels)\n",
    "            loss, _ = output\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            print(\"here 1\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            print(global_step % eval_every)\n",
    "            # evaluation step\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "\n",
    "                    # validation loop\n",
    "                    print(\"here 3\")\n",
    "                    for (title, text, titletext,labels), _ in valid_loader:\n",
    "                        labels = labels.type(torch.LongTensor)           \n",
    "                        labels = labels.to(device)\n",
    "                        titletext = titletext.type(torch.LongTensor)  \n",
    "                        titletext = titletext.to(device)\n",
    "                        output = model(titletext, labels)\n",
    "                        loss, _ = output\n",
    "                        \n",
    "                        valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                print(\"here 4\")\n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                              average_train_loss, average_valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
    "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nhere 1\n1\nhere 1\n2\nhere 1\n3\nhere 1\n4\nhere 1\n5\nhere 1\n6\nhere 1\n7\nhere 1\n8\nhere 1\n9\n"
    }
   ],
   "source": [
    "model = BERT().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-7)\n",
    "\n",
    "train(model=model, optimizer=optimizer)"
   ]
  }
 ]
}